{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+3lHznNov/2c0aL3D6hcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumant1122/PythonBootcamp/blob/main/Session4_PythonBootcamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP : Summarization"
      ],
      "metadata": {
        "id": "9oWhsX2Gf7wC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required libraries"
      ],
      "metadata": {
        "id": "24z_x-4LgDXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ofo9i-KT9q_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the file"
      ],
      "metadata": {
        "id": "O_ZZ-_DpgHfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"tennis.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0t-wYvxX-YkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['article_text'][1]"
      ],
      "metadata": {
        "id": "xD4847W-cYFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the sentences"
      ],
      "metadata": {
        "id": "Z7Cto-9fgP2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = []\n",
        "for s in df['article_text']:\n",
        "    sentences.append(sent_tokenize(s))\n",
        "\n",
        "sentences = [y for x in sentences for y in x]"
      ],
      "metadata": {
        "id": "erIUSf4vdKar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Glove Word Representation\n",
        "\n",
        "GloVe, short for Global Vectors for Word Representation, is an unsupervised learning algorithm used to generate word embeddings by analyzing global word-word co-occurrence statistics from a large text corpus.\n",
        " This method constructs a co-occurrence matrix, where each element represents how often a pair of words appears together within a specific context window.\n",
        " The algorithm then factorizes this matrix to obtain lower-dimensional vector representations for each word, capturing both semantic and syntactic relationships.\n",
        "\n"
      ],
      "metadata": {
        "id": "Il-ZIN5TgcxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()\n",
        "\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "clean_sentences = [s.lower() for s in clean_sentences]\n",
        "stop_words = stopwords.words('english')\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "metadata": {
        "id": "4GS7ON32dLQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectors for the sentences"
      ],
      "metadata": {
        "id": "s0pm4hPagy_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors.append(v)"
      ],
      "metadata": {
        "id": "-hqtiqlffUzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "similarities between the sentences"
      ],
      "metadata": {
        "id": "6zjV1GkDg0nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
      ],
      "metadata": {
        "id": "FTL11pm3ffRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a graph of about similarities"
      ],
      "metadata": {
        "id": "reCrGC3xg9ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "metadata": {
        "id": "BTsiGI74fgMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Output"
      ],
      "metadata": {
        "id": "v-lzA9TZhHT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "for i in range(5):\n",
        "  print(\"ARTICLE:\")\n",
        "  print(df['article_text'][i])\n",
        "  print('\\n')\n",
        "  print(\"SUMMARY:\")\n",
        "  print(ranked_sentences[i][1])\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "M_Ur0Wdefk-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace Summarization\n"
      ],
      "metadata": {
        "id": "2kCmzxWZiorp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_example = '''The tower is 324 meters (1,063 ft) tall, about the same height\n",
        "as an 81-storey building, and the tallest structure in Paris. Its base is square,\n",
        "measuring 125 meters (410 ft) on each side. During its construction, the Eiffel\n",
        "Tower surpassed the Washington Monument to become the tallest man-made structure\n",
        "in the world, a title it held for 41 years until the Chrysler Building in New York\n",
        "City was finished in 1930. It was the first structure to reach a height of 300 meters.\n",
        "Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is\n",
        "now taller than the Chrysler Building by 5.2 meters (17 ft). Excluding transmitters,\n",
        "the Eiffel Tower is the second tallest free-standing structure in France\n",
        "after the Millau Viaduct.'''"
      ],
      "metadata": {
        "id": "ZMVWOATokHdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model = \"google/pegasus-cnn_dailymail\")\n",
        "summarizer(text_example)"
      ],
      "metadata": {
        "id": "_ZnlnaUFjk8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n",
        "summarizer(text_example)"
      ],
      "metadata": {
        "id": "ImfYYVAVlghu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "oNAQtKwmsPNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"There is a sale in supermarket\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ],
      "metadata": {
        "id": "5X_D3Qf8y8hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-classification\")\n",
        "pipe(\"I hate the food here\")"
      ],
      "metadata": {
        "id": "NeBYB7r8sOmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "#classifier(\"This movie is disgustingly good !\")\n",
        "\n",
        "classifier(\"Director tried too much.\")"
      ],
      "metadata": {
        "id": "mWQaLtq-tHuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ],
      "metadata": {
        "id": "P_IqjgEAzKpB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}